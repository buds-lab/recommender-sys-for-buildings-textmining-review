{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation here https://dev.elsevier.com/documentation/ArticleRetrievalAPI.wadl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.elsevier.com/content/article/doi/{doi}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import xmltodict\n",
    "import json\n",
    "from credentials import keys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Repeat Items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'sciencedirect_search_results.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print the original number of rows for comparison\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "\n",
    "# Remove duplicates based on the 'DOI' column\n",
    "df_cleaned = df.drop_duplicates(subset=['DOI'])\n",
    "\n",
    "# Print the number of rows after removing duplicates\n",
    "print(f\"Number of rows after removing duplicates: {len(df_cleaned)}\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "cleaned_file_path = 'elsevier_search_results_cleaned.csv'\n",
    "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file\n",
    "df = pd.read_csv('elsevier_search_results_cleaned.csv')\n",
    "\n",
    "# Creating a folder to save the downloaded articles\n",
    "download_dir = 'downloaded_articles'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# Getting the total number of articles to download\n",
    "total_articles = len(df)\n",
    "\n",
    "# Initializing the counter for downloaded articles\n",
    "downloaded_articles = 0\n",
    "\n",
    "# Iterating through the list of DOIs\n",
    "for index, row in df.iterrows():\n",
    "    doi = row['DOI']  # Assuming there's a 'doi' column in the CSV file\n",
    "    url = f\"https://api.elsevier.com/content/article/doi/{doi}\"\n",
    "    \n",
    "    # Sending the request\n",
    "    response = requests.get(url, headers={\n",
    "        \"X-ELS-APIKey\": keys[\"els-apikey\"],\n",
    "        \"X-ELS-Insttoken\": keys[\"els-inst-token\"],\n",
    "        \"Accept\": \"application/json\"\n",
    "    })\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Increment the counter if the download is successful\n",
    "        downloaded_articles += 1\n",
    "        article_data = response.json()\n",
    "        filename = doi.replace('/', '_') + '.json'\n",
    "        file_path = os.path.join(download_dir, filename)\n",
    "        \n",
    "        # Saving the article data to a file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "    # Use '\\r' to return to the start of the line and 'end=\"\"' to prevent new line. Flush to ensure it's displayed immediately.\n",
    "    print(f\"\\rDownloaded: {downloaded_articles}/{total_articles}\", end='', flush=True)\n",
    "\n",
    "# Adding a new line at the end of the process to ensure the command prompt appears correctly after the script finishes.\n",
    "print(\"\\nAll articles processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Error Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path that contains your JSON files\n",
    "directory_path = 'C:/Users/wenha/OneDrive - University College London/Desktop/first_paper_code/downloaded_articles'\n",
    "\n",
    "# Loop through each file in the specified directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    # Construct the full path of the file\n",
    "    file_path = os.path.join(directory_path, filename)\n",
    "    # Check if the file is a JSON file\n",
    "    if filename.endswith('.json'):\n",
    "        # Get the size of the file in kilobytes (KB)\n",
    "        file_size_kb = os.path.getsize(file_path) / 1024\n",
    "        # Check if the file size is less than 6KB\n",
    "        if file_size_kb < 7:\n",
    "            # Delete the file\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete file without 'originalText'\n",
    "import os\n",
    "import json\n",
    "\n",
    "def process_json_files(directory):\n",
    "    error_files = []\n",
    "\n",
    "    # Iterate through each JSON file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                \n",
    "                # Extract the original text from the JSON structure\n",
    "                text = data['full-text-retrieval-response']['originalText']\n",
    "                # Attempt to convert to lowercase to check for errors\n",
    "                _ = text.lower()\n",
    "            except AttributeError:\n",
    "                error_files.append(filename)\n",
    "                # Delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {filename} due to an AttributeError.\")\n",
    "            except KeyError:\n",
    "                print(f\"KeyError: 'originalText' not found in {filename}\")\n",
    "    \n",
    "    return error_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path\n",
    "directory = 'C:/Users/wenha/OneDrive - University College London/Desktop/first_paper_code/downloaded_articles'\n",
    "\n",
    "# Run the processing function\n",
    "error_files = process_json_files(directory)\n",
    "\n",
    "# Print the problematic files\n",
    "#print(\"Deleted Error Files:\")\n",
    "#for file in error_files:\n",
    "    #print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
